import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.packs.raptor import RaptorRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from models.ai_models import AIModels
from prompts.prompts import search_query
from time import sleep
from tenacity import retry, stop_after_attempt, wait_exponential
import fitz  # PyMuPDF
from typing import Dict, Any

class RaptorSetup:
    """Configuration et initialisation de Raptor"""
    def __init__(self, ai_models: AIModels):
        print("\nüîß Initialisation de ChromaDB...")
        try:
            # Initialisation de la base de donn√©es
            self.client = chromadb.PersistentClient(path="./RAPTOR_db")
            print("‚úÖ Client ChromaDB cr√©√© avec succ√®s")
            
            # V√©rifier si la collection existe d√©j√†
            collection_names = self.client.list_collections()
            print(f"üìö Collections existantes : {collection_names}")
            
            # V√©rifier si notre collection existe
            if "legislation_PUB" in [c.name for c in collection_names]:
                print("üìö Collection 'legislation_PUB' trouv√©e")
                self.collection = self.client.get_collection("legislation_PUB")
            else:
                print("üìö Cr√©ation de la collection 'legislation_PUB'")
                self.collection = self.client.create_collection("legislation_PUB")
            
            print(f"‚úÖ Collection 'legislation_PUB' initialis√©e - Nombre d'√©l√©ments : {self.collection.count()}")
            
            self.vector_store = ChromaVectorStore(chroma_collection=self.collection)
            print("‚úÖ Vector store initialis√©")
            
            # Initialisation du retriever avec un cache
            print("\nüîÑ Configuration du retriever Raptor...")
            self.retriever = RaptorRetriever(
                [],
                embed_model=ai_models.embedding_model,
                llm=ai_models.llm,
                vector_store=self.vector_store,
                similarity_top_k=5,
                mode="collapsed",
                verbose=True
            )
            print("‚úÖ Retriever configur√©")
            
            # Cache pour les r√©sultats de recherche
            self._search_cache = {}
            
            # Initialisation du query engine
            print("\nüîÑ Configuration du query engine...")
            self.query_engine = RetrieverQueryEngine.from_args(
                self.retriever,
                llm=ai_models.llm
            )
            print("‚úÖ Query engine configur√©")
            
        except Exception as e:
            print(f"\n‚ùå Erreur lors de l'initialisation de ChromaDB : {str(e)}")
            print("üìù D√©tails de l'erreur :")
            import traceback
            print(traceback.format_exc())
            raise
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def search(self, query: str) -> str:
        """
        Recherche la l√©gislation applicable dans la base de connaissances
        
        Args:
            query: Contexte de la recherche
            
        Returns:
            str: Textes de loi trouv√©s
        """
        # V√©rifier le cache
        if query in self._search_cache:
            print("\nüìö Utilisation du cache pour la recherche...")
            return self._search_cache[query]
        
        print(f"\nüìö Recherche de l√©gislation pour: {query[:200]}...")
        
        # Construire la requ√™te de recherche
        formatted_query = search_query.format(query=query)
        print(f"\nRequ√™te format√©e: {formatted_query[:200]}...")
        
        try:
            print("\nüîç D√©but de la recherche dans ChromaDB...")
            print(f"üìä Nombre d'√©l√©ments dans la collection : {self.collection.count()}")
            
            # R√©cup√©rer les documents pertinents avec retry
            print("üîÑ Ex√©cution de la requ√™te via le retriever...")
            results = self.retriever.retrieve(formatted_query)
            print(f"‚úÖ Requ√™te ex√©cut√©e - Nombre de r√©sultats : {len(results)}")
            
            # Extraire le texte des r√©sultats
            text_results = []
            for i, node in enumerate(results, 1):
                print(f"\nüìÑ Traitement du r√©sultat {i}/{len(results)}")
                if hasattr(node, 'text'):
                    text_results.append(node.text)
                    print(f"‚úÖ Texte extrait (longueur: {len(node.text)} caract√®res)")
                elif hasattr(node, 'content'):
                    text_results.append(node.content)
                    print(f"‚úÖ Contenu extrait (longueur: {len(node.content)} caract√®res)")
            
            result_text = "\n".join(text_results) if text_results else "Aucune l√©gislation trouv√©e."
            print(f"\nüìù R√©sultat final - Longueur totale : {len(result_text)} caract√®res")
            
            # Mettre en cache le r√©sultat
            self._search_cache[query] = result_text
            print("‚úÖ R√©sultat mis en cache")
            
            # Attendre entre les requ√™tes
            sleep(2)
            
            return result_text
            
        except Exception as e:
            print(f"\n‚ùå Erreur lors de la recherche : {str(e)}")
            print("üìù D√©tails de l'erreur :")
            import traceback
            print(traceback.format_exc())
            raise

    def query(self, query_text: str) -> str:
        """
        Ex√©cute une requ√™te via le query engine
        
        Args:
            query_text: La question √† poser
            
        Returns:
            str: La r√©ponse g√©n√©r√©e
        """
        print(f"\nüìö Ex√©cution de la requ√™te Raptor: {query_text[:200]}...")
        try:
            # Utiliser directement les r√©sultats de la recherche pr√©c√©dente si disponible
            if hasattr(self, '_last_search_results'):
                print("üí° Utilisation des r√©sultats de recherche pr√©c√©dents")
                response = self.llm.complete(query_text + "\n\nContexte:\n" + self._last_search_results)
                return str(response)
            
            # Sinon, utiliser le query engine
            print("üîÑ Utilisation du query engine...")
            response = self.query_engine.query(query_text)
            print("‚úÖ R√©ponse g√©n√©r√©e")
            return str(response)
            
        except Exception as e:
            print(f"\n‚ùå Erreur lors de la requ√™te Raptor: {str(e)}")
            print("üìù D√©tails de l'erreur :")
            import traceback
            print(traceback.format_exc())
            raise

    def extract_pdf_content(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extrait le contenu d'un PDF pour l'agent React
        
        Args:
            pdf_path: Chemin vers le PDF
            
        Returns:
            Dict[str, Any]: Contenu structur√© du PDF
        """
        try:
            # Ouvrir le PDF
            pdf = fitz.open(pdf_path)
            
            # Extraire le contenu
            content = {
                "type": "pdf",
                "pages": [],
                "metadata": {
                    "total_pages": len(pdf),
                    "filename": pdf_path
                }
            }
            
            # Extraire chaque page
            for page_num in range(len(pdf)):
                page = pdf[page_num]
                
                # Extraire texte et images
                text = page.get_text()
                
                # Ajouter la page au contenu
                content["pages"].append({
                    "page_number": page_num + 1,
                    "content": text
                })
            
            print(f"üìÑ PDF extrait : {len(content['pages'])} pages")
            return content
            
        except Exception as e:
            print(f"\n‚ùå Erreur lors de l'extraction du PDF : {str(e)}")
            raise
        finally:
            if 'pdf' in locals():
                pdf.close() 